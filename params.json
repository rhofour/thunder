{"name":"Thunder","tagline":"Large-scale neural data analysis with Spark","body":"# Factor Analysis in Thunder (A CMU 15-418 Final Project)\r\n_by Richard Alex Hofer (rhofer) and Owen Kahn (okahn)_\r\nOur goal for this project is to implement factor analysis in the distributed machine learning framework Thunder.\r\n\r\n## Background\r\nModern data analysis (especially of neural data) often involves data with 100+ dimensions which are difficult to analyze as-is. To makes sense of this high dimensional data we often assume it comes from a much lower dimensional data. To find this lower dimensional data we perform one of several dimensionality reduction algorithms such as Factor Analysis.\r\n\r\nThere are already several tools available for performing factor analysis, however we're unaware of any which scale well when the data we're interested in doesn't fit in memory on a single machine. However, with newer methods of data collection such as optogenetic methods we're easily able to collect datasets on the order of 100GB-1TB.\r\n\r\nThunder is a data analysis framework built on top of Apache Spark designed for working with these kinds of datasets. The idea behind using Thunder is that if we can keep the data we're interested in distributed across the main memory of several machines and avoid reading and writing to disk then we can perform analyses much in much less time.\r\n\r\n## The Challenge\r\nWhile we believe using multiple machines to keep our data in memory will offer a substantial speed up to this algorithm, it will also significantly increases the communication costs we have.\r\n\r\nAnother challenge we will face is in how to split this workload up among multiple machines. As far as we're aware there is no trivial way to divide the work when building a factor analysis model so we may have to devise several different ways to do this and figure out which works best.\r\n\r\n## Resources\r\nSince our algorithm is being implemented inside of Thunder we definitely intend to take advantage of any Thunder or Spark primitives and functions which make our job easier.\r\n\r\nIdeally we hope to test our algorithm with real neural data from Prof. Byron Yu. However, if we're unable to get real data to test with we can easily generate synthetic data instead since factor analysis is a generative model.\r\n\r\nWe have few requirements for the machines we test on so long as they can communicate reasonably quickly with each other and are able to read in our large dataset.\r\n\r\n## Goals and Deliverables\r\n### Plan to Achieve\r\nAn implementation of factor analysis in Thunder which is able to outperform an optimized implementation on a single machine when the data being analyzed doesn't fit in memory. We suspect any reasonably efficient implementation that keeps all of the data in memory will be able to achieve this because of the major increase in latency when reading from disk.\r\n\r\n### Hope to Achieve\r\nIdeally we would like to see increasing speedup as we add more machines. This may be difficult to achieve as it requires additional communication, however it is conceivably that to a certain point adding additional machines could improve performance if all of them are able to do significant work.\r\n\r\n## Platform Choice\r\nAs discussed in Resources we can implement this on either EC2, Latedays, or some other cluster.\r\n\r\nBuilding our algorithm on top of Thunder is a reasonable choice because Thunder was specifically made for efficiently running neural data analysis algorithms like Factor Analysis.\r\n\r\n## Schedule\r\nApril  2\r\n* Have our proposal submitted\r\n\r\nApril  5\r\n* Have Thunder built successfully on our local machines\r\n\r\nApril 12\r\n* Have a clear plan on which implementation of factor analysis we hope to pursue\r\n* Have an optimized single-machine algorithm setup (not implemented by us)\r\n* Have started on our actual implementation\r\n\r\nApril 19\r\n* Have our algorithm correctly implemented (though possibly not performant)","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}