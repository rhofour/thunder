{"name":"Thunder","tagline":"Large-scale neural data analysis with Spark","body":"# Matrix Inverse in Thunder (A CMU 15-418 Final Project)\r\n_by Richard Alex Hofer (rhofer) and Owen Kahn (okahn)_\r\n\r\nOur goal for this project is to implement distributed matrix inverse.\r\n\r\n_Note: This was not our original project_\r\n\r\n## Final writeup\r\nOur writeup is avaiable [here](https://docs.google.com/document/d/133MqM6W5rxogC_GBUtrMzwvLQ1jZi0Il9C1Svjaj3V8/edit?usp=sharing)\r\n\r\n## Progress Report\r\nOur progress report is available [here](https://github.com/rhofour/thunder/wiki/Project-Checkpoint)\r\n\r\n## Background\r\nMatrices are important. Lots of frameworks built on top of Spark provide distributed matrices and a few operations, but it appears that none of them offer a matrix inverse which can be very useful. This is likely a sign that it is a very difficult operation to implement and we probably should have paid more attention to this.\r\n\r\n## The Challenge\r\nMatrix inverse does not distribute easily and computing any part of it generally requires access to many parts of the original matrix. This communication makes its implementation difficult when working with distributed matrices. It also does not lend itself well to Spark primitives.\r\n\r\n## Resources\r\nWe will test on Amazon EC2 because it is easy to deploy Thunder there and it allows us to easily test on many different cluster configurations.\r\n\r\n## Goals and Deliverables\r\n### Plan to Achieve\r\nA correct implementation of distributed matrix inverse which is faster than using a single machine (which would be forced to read and write from disk).\r\n\r\n### Hope to Achieve\r\nIdeally we would like to be competitive with the implementation provided by the paper we our working from.\r\n\r\n## Platform Choice\r\nAs discussed in Resources we will use Amazon EC2.\r\n\r\n## Schedule\r\nApril  25\r\n* Have a correct implementation ✓\r\n\r\nMay 2\r\n* Run our implementation on EC2 ✓\r\n\r\nMay 3\r\n* Run reference implementation on EC2 ✓\r\n\r\nMay 6\r\n* Develop tests and upload to S3 ✓\r\n\r\nMay 8\r\n* Investigate unreasonable memory requirements (ongoing)\r\n* Contact the Spark community to learn about Spark internals ✓\r\n\r\nMay 10\r\n* Have presentation prepared\r\n\r\n## Results\r\nWe have a correct implementation of inverse which falls apart if run on a larger cluster due to unidentified overhead internal to Spark. We're attempting to figure out why this occurs, but are having difficulties since we cannot find anyone who is doing a similar type of computation.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}